import cv2
import numpy as np
import os
import re
import logging
import matplotlib
matplotlib.use('Agg')  # GUI olmayan ortamlar iÃ§in
import matplotlib.pyplot as plt
from trajectory import OptimizedTrajectoryTracker
from kalman import KalmanFilter
from utils import (plot_trajectory, plot_trajectory_comparison_2d, print_trajectory_stats,
                   load_ground_truth_csv, calibrate_scale_3d, get_camera_matrix, get_distortion_coeffs,
                   plot_3d_trajectory_comparison)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def frame_number(filename):
    """Dosya adÄ±ndan frame numarasÄ±nÄ± Ã§Ä±kar"""
    match = re.search(r'(\d+)', filename)
    return int(match.group(1)) if match else -1

def load_ground_truth(file_path):
    """Ground truth dosyasÄ±nÄ± yÃ¼kle"""
    if not file_path or not os.path.exists(file_path):
        logging.warning(f"Ground truth dosyasÄ± bulunamadÄ±: {file_path}")
        return None
    
    try:
        gt = np.genfromtxt(file_path, delimiter=',', skip_header=1, usecols=(0,1,2), names=['x', 'y', 'z'])
        logging.info(f"Ground truth baÅŸarÄ±yla yÃ¼klendi: {file_path}")
        return np.column_stack((gt['x'], gt['y'], gt['z']))
    except Exception as e:
        logging.warning(f"Ground truth yÃ¼klenemedi: {e}")
        return None

# --- Kamera parametreleri ---
CAMERA_INTRINSICS = {
    'K': np.array([[2792.2, 0, 1988.0],
                  [0, 2795.2, 1562.2],
                  [0, 0, 1]]),
    'dist': np.array([0.0798, -0.1867, 0, 0])
}

def extract_features(img, method='SIFT'):
    """
    GÃ¶rÃ¼ntÃ¼den Ã¶zellikleri Ã§Ä±kartÄ±r (SIFT veya ORB)
    """
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
    
    # GÃ¶rÃ¼ntÃ¼ iyileÅŸtirme
    gray = cv2.equalizeHist(gray)  # Kontrast iyileÅŸtirme
    gray = cv2.GaussianBlur(gray, (3, 3), 0)  # GÃ¼rÃ¼ltÃ¼ azaltma
    
    if method == 'SIFT':
        detector = cv2.SIFT_create(nfeatures=2000, contrastThreshold=0.04)
        keypoints, descriptors = detector.detectAndCompute(gray, None)
    elif method == 'ORB':
        detector = cv2.ORB_create(nfeatures=2000, scaleFactor=1.2)
        keypoints, descriptors = detector.detectAndCompute(gray, None)
    else:
        raise ValueError(f"Desteklenmeyen Ã¶zellik Ã§Ä±karÄ±m yÃ¶ntemi: {method}")
    
    return keypoints, descriptors

def match_features(desc1, desc2, method='FLANN', detector='SIFT'):
    """
    Ä°ki gÃ¶rÃ¼ntÃ¼ arasÄ±nda Ã¶zellik eÅŸleÅŸtirme yapar (FLANN veya BFMatcher)
    """
    if desc1 is None or desc2 is None:
        return []
    
    if method == 'BF':
        # Brute Force eÅŸleÅŸtirici
        if detector == 'ORB':
            matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        else:
            matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)
        
        # kNN eÅŸleÅŸtirme ile ratio test
        matches = matcher.knnMatch(desc1, desc2, k=2)
        good_matches = []
        for m, n in matches:
            if m.distance < 0.7 * n.distance:  # Lowe's ratio test
                good_matches.append(m)
    else:
        # FLANN eÅŸleÅŸtirici
        if detector == 'ORB':
            index_params = dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1)
        else:
            index_params = dict(algorithm=1, trees=5)
        
        search_params = dict(checks=50)
        matcher = cv2.FlannBasedMatcher(index_params, search_params)
        
        matches = matcher.knnMatch(desc1, desc2, k=2)
        good_matches = []
        for m_n in matches:
            if len(m_n) >= 2:
                m, n = m_n
                if m.distance < 0.7 * n.distance:
                    good_matches.append(m)
    
    return good_matches

def filter_matches_ransac(kp1, kp2, matches, K):
    """
    RANSAC ile eÅŸleÅŸmeleri filtrele ve Essential Matrix hesapla
    """
    if len(matches) < 8:  # Essential matrix iÃ§in en az 8 nokta gerekir
        return None, None, None
    
    # EÅŸleÅŸen noktalarÄ± al
    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])
    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])
    
    # RANSAC ile Essential Matrix hesapla
    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)
    
    if E is None or mask is None:
        return None, None, None
    
    # Sadece inlier noktalarÄ± seÃ§
    pts1_inliers = pts1[mask.ravel() == 1]
    pts2_inliers = pts2[mask.ravel() == 1]
    
    return E, pts1_inliers, pts2_inliers

def recover_pose_from_essential(E, pts1, pts2, K):
    """
    Essential matrixten kamera pozunu hesapla (R, t)
    """
    # recoverPose ile rotasyon ve translasyon hesapla
    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)
    
    return R, t, mask

def visualize_matches(img1, img2, kp1, kp2, matches, title="Feature Matches"):
    """
    EÅŸleÅŸen Ã¶zellikleri gÃ¶rselleÅŸtir
    """
    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:100], None, 
                                flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    
    cv2.imshow(title, img_matches)
    cv2.waitKey(1)

def process_video_sequence(image_folder, ground_truth_file=None, 
                           output_folder="results", feature_method='SIFT', 
                           matching_method='FLANN', use_kalman=True):
    """
    GÃ¶rÃ¼ntÃ¼ dizisini iÅŸleyerek kamera hareketini takip eder
    
    1. GÃ¶rÃ¼ntÃ¼leri Oku ve Ã–n Ä°ÅŸlem
    2. Ã–zellik Ã‡Ä±karÄ±mÄ± (ORB/SIFT)
    3. Ã–zellik EÅŸleÅŸtirme (BFMatcher/FLANN + RANSAC)
    4. Essential Matrix Hesapla
    5. Kamera Hareketini (Pose) Bul
    6. Kalman Filtresi ile Trajektori DÃ¼zeltme
    7. SonuÃ§larÄ± Kaydet/GÃ¶ster
    """
    logging.info("ğŸš€ Visual Odometry baÅŸlatÄ±lÄ±yor...")
    
    # Ã‡Ä±ktÄ± klasÃ¶rÃ¼nÃ¼ oluÅŸtur
    os.makedirs(output_folder, exist_ok=True)
    
    # Kamera parametreleri
    K = CAMERA_INTRINSICS['K']
    dist = CAMERA_INTRINSICS['dist']
    
    # Kalman filtresi iÃ§in
    kalman = KalmanFilter(dt=1.0, process_noise=1e-4, measurement_noise=1e-2)
    
    # GÃ¶rÃ¼ntÃ¼leri yÃ¼kle
    frames = sorted([f for f in os.listdir(image_folder) 
                    if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))],
                   key=frame_number)
    
    if len(frames) < 2:
        logging.error("En az 2 gÃ¶rÃ¼ntÃ¼ gerekli!")
        return None
    
    logging.info(f"ğŸ“ Toplam {len(frames)} gÃ¶rÃ¼ntÃ¼ yÃ¼klendi.")
    logging.info(f"ğŸ”§ Ã–zellik Ã§Ä±karÄ±m yÃ¶ntemi: {feature_method}")
    logging.info(f"ğŸ”§ EÅŸleÅŸtirme yÃ¶ntemi: {matching_method}")
    logging.info(f"ğŸ”§ Kalman filtresi: {'Aktif' if use_kalman else 'Pasif'}")
    
    # Trajektori iÃ§in
    positions = []  # Kamera pozisyonlarÄ±
    rotations = []  # Kamera rotasyonlarÄ±
    
    # BaÅŸlangÄ±Ã§ pozisyon ve rotasyon
    current_R = np.eye(3)  # BaÅŸlangÄ±Ã§ rotasyonu (identity matrix)
    current_t = np.zeros((3, 1))  # BaÅŸlangÄ±Ã§ konumu (sÄ±fÄ±r vektÃ¶r)
    
    # Ä°lk pozisyonu ekle
    positions.append(current_t.flatten())
    rotations.append(current_R.copy())
    
    prev_img = None
    prev_kp = None
    prev_desc = None
    
    successful_poses = 0
    failed_poses = 0
    
    for idx, frame_name in enumerate(frames):
        path = os.path.join(image_folder, frame_name)
        curr_img = cv2.imread(path)
        
        if curr_img is None:
            logging.warning(f"GÃ¶rÃ¼ntÃ¼ aÃ§Ä±lamadÄ±: {path}, atlanÄ±yor.")
            continue
        
        # Distorsiyon dÃ¼zeltme
        curr_img = cv2.undistort(curr_img, K, dist)
        
        # 1. Ã–zellik Ã‡Ä±karÄ±mÄ±
        curr_kp, curr_desc = extract_features(curr_img, method=feature_method)
        
        # Ä°lk kare iÃ§in sadece Ã¶zellikleri sakla ve devam et
        if prev_img is None:
            prev_img = curr_img
            prev_kp = curr_kp
            prev_desc = curr_desc
            logging.info(f"Ä°lk kare hazÄ±rlandÄ±: {frame_name}")
            continue
        
        # 2. Ã–zellik EÅŸleÅŸtirme
        if prev_desc is not None and curr_desc is not None and len(prev_kp) > 10 and len(curr_kp) > 10:
            matches = match_features(prev_desc, curr_desc, method=matching_method, detector=feature_method)
            
            # Yeterli eÅŸleÅŸme varsa devam et
            if len(matches) >= 8:
                # 3. RANSAC ile eÅŸleÅŸmeleri filtrele ve Essential Matrix hesapla
                E, pts1, pts2 = filter_matches_ransac(prev_kp, curr_kp, matches, K)
                
                if E is not None and pts1 is not None and pts2 is not None:
                    # 4. Essential Matrix'ten kamera hareketini (R, t) bul
                    R, t, _ = recover_pose_from_essential(E, pts1, pts2, K)
                    
                    # 5. Hareketi kÃ¼mÃ¼latif olarak hesapla
                    current_t = current_t + current_R @ t
                    current_R = R @ current_R
                    
                    # 6. Kalman Filtresi ile dÃ¼zeltme (opsiyonel)
                    if use_kalman:
                        current_t_filtered = kalman.predict_and_update(current_t.flatten())
                        current_t = current_t_filtered.reshape(3, 1)
                    
                    # Yeni pozisyonu ve rotasyonu kaydet
                    positions.append(current_t.flatten())
                    rotations.append(current_R.copy())
                    
                    successful_poses += 1
                    
                    # Her 10 karede bir gÃ¶rselleÅŸtirme
                    if idx % 10 == 0:
                        visualize_matches(prev_img, curr_img, prev_kp, curr_kp, matches, 
                                         title=f"Frame {idx}: {len(matches)} matches")
                else:
                    logging.warning(f"Frame {idx}: Essential Matrix hesaplanamadÄ±")
                    positions.append(current_t.flatten())  # Son pozisyonu tekrar ekle
                    rotations.append(current_R.copy())
                    failed_poses += 1
            else:
                logging.warning(f"Frame {idx}: Yeterli eÅŸleÅŸme bulunamadÄ± ({len(matches)} < 8)")
                positions.append(current_t.flatten())  # Son pozisyonu tekrar ekle
                rotations.append(current_R.copy())
                failed_poses += 1
        else:
            logging.warning(f"Frame {idx}: Yeterli Ã¶zellik bulunamadÄ±")
            positions.append(current_t.flatten())  # Son pozisyonu tekrar ekle
            rotations.append(current_R.copy())
            failed_poses += 1
        
        # Sonraki iterasyon iÃ§in
        prev_img = curr_img
        prev_kp = curr_kp
        prev_desc = curr_desc
        
        # Ä°lerleme durumu
        if idx % 100 == 0 and idx > 0:
            success_rate = (successful_poses / (successful_poses + failed_poses)) * 100
            logging.info(f"Ä°ÅŸlenen: {idx+1}/{len(frames)} - BaÅŸarÄ± oranÄ±: {success_rate:.1f}%")
    
    cv2.destroyAllWindows()
    
    # SonuÃ§larÄ± numpy array'e dÃ¶nÃ¼ÅŸtÃ¼r
    trajectory = np.array(positions)
    
    if len(trajectory) == 0:
        logging.error("HiÃ§ poz tahmini yapÄ±lmadÄ±.")
        return None
    
    # Ground truth ile karÅŸÄ±laÅŸtÄ±rma ve Ã¶lÃ§eklendirme
    if ground_truth_file and os.path.exists(ground_truth_file):
        logging.info("Ground truth ile karÅŸÄ±laÅŸtÄ±rma yapÄ±lÄ±yor...")
        
        # Ground truth verisini yÃ¼kle - doÄŸrudan CSV'den
        gt_positions, gt_frame_indices = load_ground_truth_csv(ground_truth_file)
        
        if gt_positions is not None and len(gt_positions) > 0:
            # Ã–lÃ§eklendirme
            trajectory_scaled, scale = calibrate_scale_3d(trajectory, gt_positions, gt_frame_indices)
            logging.info(f"Ã–lÃ§eklendirme faktÃ¶rÃ¼: {scale:.4f}")
            
            # Hata hesapla
            from utils import calculate_trajectory_errors
            error_stats = calculate_trajectory_errors(trajectory_scaled, gt_positions, gt_frame_indices)
            
            if error_stats:
                logging.info(f"3D ortalama hata: {error_stats['3d_mean']:.4f}m")
                logging.info(f"2D ortalama hata: {error_stats['2d_mean']:.4f}m")
            
            # GÃ¶rselleÅŸtirme
            plot_3d_trajectory_comparison(
                trajectory_scaled[:min(len(trajectory_scaled), len(gt_positions))],
                gt_positions[:min(len(trajectory_scaled), len(gt_positions))],
                title="Trajectory Comparison",
                save_path=os.path.join(output_folder, "trajectory_comparison.png")
            )
        else:
            trajectory_scaled = trajectory
            scale = 1.0
            logging.warning("Ground truth verisi yÃ¼klenemedi veya karÅŸÄ±laÅŸtÄ±rma yapÄ±lamadÄ±.")
    else:
        trajectory_scaled = trajectory
        scale = 1.0
        logging.info("Ground truth verisi bulunamadÄ±, sadece tahmin edilen trajektori kaydediliyor.")
    
    # SonuÃ§larÄ± kaydet
    np.savetxt(os.path.join(output_folder, "estimated_trajectory.txt"), trajectory_scaled, fmt="%.6f")
    logging.info(f"Trajektori kaydedildi: {os.path.join(output_folder, 'estimated_trajectory.txt')}")
    
    # Ä°statistikler
    total_distance = 0
    for i in range(1, len(trajectory_scaled)):
        total_distance += np.linalg.norm(trajectory_scaled[i] - trajectory_scaled[i-1])
    
    success_rate = (successful_poses / (successful_poses + failed_poses)) * 100 if (successful_poses + failed_poses) > 0 else 0
    
    logging.info(f"Ä°ÅŸlem tamamlandÄ±:")
    logging.info(f"  - Toplam kare: {len(frames)}")
    logging.info(f"  - BaÅŸarÄ±lÄ± pozlar: {successful_poses}")
    logging.info(f"  - BaÅŸarÄ±sÄ±z pozlar: {failed_poses}")
    logging.info(f"  - BaÅŸarÄ± oranÄ±: {success_rate:.1f}%")
    logging.info(f"  - Toplam mesafe: {total_distance:.4f}m")
    
    # SonuÃ§larÄ± gÃ¶rselleÅŸtir
    plot_trajectory(trajectory_scaled, 
                   title=f"Estimated Trajectory (Total distance: {total_distance:.2f}m)", 
                   save_path=os.path.join(output_folder, "trajectory_3d.png"))
    
    return trajectory_scaled

def main():
    """
    Ana fonksiyon - TÃ¼m parametreler burada tanÄ±mlÄ±, terminal parametresi gerektirmez
    """
    # Parametreler - Bu deÄŸerleri kendi projenize gÃ¶re deÄŸiÅŸtirin
    image_folder = "C:\\Users\\zeyne\\Desktop\\termal calÄ±smalarÄ±\\termal1_frames"
    ground_truth_file = "C:\\Users\\zeyne\\Desktop\\termal calÄ±smalarÄ±\\termal1.csv"
    output_folder = "C:\\Users\\zeyne\\Desktop\\termal calÄ±smalarÄ±\\results"
    
    # DiÄŸer parametreler
    feature_method = 'SIFT'  # 'SIFT' veya 'ORB'
    matching_method = 'FLANN'  # 'FLANN' veya 'BF'
    use_kalman = True
    
    print("ğŸš€ Visual Odometry Sistemi")
    print("=" * 50)
    print(f"ğŸ“ GÃ¶rÃ¼ntÃ¼ klasÃ¶rÃ¼: {image_folder}")
    print(f"ğŸ“Š Ground truth: {ground_truth_file}")
    print(f"ğŸ“ SonuÃ§ klasÃ¶rÃ¼: {output_folder}")
    print(f"ğŸ”§ Ã–zellik Ã§Ä±karÄ±mÄ±: {feature_method}")
    print(f"ğŸ”§ EÅŸleÅŸtirme yÃ¶ntemi: {matching_method}")
    print(f"ï¿½ Kalman filtresi: {'Aktif' if use_kalman else 'Pasif'}")
    print("=" * 50)
    
    try:
        # Ä°ÅŸlemi baÅŸlat
        trajectory = process_video_sequence(
            image_folder=image_folder,
            ground_truth_file=ground_truth_file,
            output_folder=output_folder,
            feature_method=feature_method,
            matching_method=matching_method,
            use_kalman=use_kalman
        )
        
        if trajectory is not None:
            print("\nâœ… Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
            print(f"ğŸ“ˆ Toplam {len(trajectory)} trajektori noktasÄ±")
            print(f"ğŸ“‚ SonuÃ§lar: {output_folder}/")
            
            return 0
        else:
            print("\nâš ï¸ Ä°ÅŸlem tamamlandÄ± ancak trajektori oluÅŸturulamadÄ±.")
            return 1
            
    except Exception as e:
        logging.error(f"Ana hata: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())